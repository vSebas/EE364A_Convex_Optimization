\section*{A9.10 True or False}

\begin{enumerate}[label=(\alph*)]
    \item In descent methods, the particular choice of search direction does not matter so much.

    \textbf{False}

    \item In descent methods, the particular choice of line search does not matter so much.

    \textbf{True}

    \item When the gradient method is started from a point near the solution, it will converge very quickly.

    \textbf{False}

    \item When Newton's method is started from a point near the solution, it will converge very quickly.

    \textbf{True}

    \item Newton's method with step size $h = 1$ always works; damping (i.e., using $h < 1$) is used only to improve the speed of convergence.

    \textbf{False}

    \item Using the gradient method to minimize $f(Ty)$, where $Ty = x$ and $T$ is nonsingular, can greatly improve the convergence speed when $T$ is chosen appropriately.

    \textbf{True}

    \item Using Newton's method to minimize $f(Ty)$, where $Ty = x$ and $T$ is nonsingular, can greatly improve the convergence speed when $T$ is chosen appropriately.

    \textbf{False}
\end{enumerate}
